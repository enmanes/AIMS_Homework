{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtFBgj623FS"
      },
      "source": [
        "# Accessing OpenAI Like a Developer (Bonus Assignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pa34dMvQ6Ai"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "If you look at the Table of Contents (accessed through the menu on the left) - you'll see this:\n",
        "\n",
        "![image](https://i.imgur.com/I8iDTUO.png)\n",
        "\n",
        "Or this if you're in Colab:\n",
        "\n",
        "![image](https://i.imgur.com/0rHA1yF.png)\n",
        "\n",
        "You'll notice during assignments that we have two following categories:\n",
        "\n",
        "1. ❓ - Questions. These will involve...answering questions!\n",
        "2. 🏗️ - Activities. These will involve writing code, or modifying text.\n",
        "\n",
        "In order to receive full marks on the assignment - it is expected you will answer all questions, and complete all activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w4egfB274VD"
      },
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "The first thing we'll do is load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23H7TMOM4mfy",
        "outputId": "3fe8126e-198a-4a8d-8db8-5329e6541641"
      },
      "outputs": [],
      "source": [
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKD8XBTVEAOw"
      },
      "source": [
        "## 2. Setting Environment Variables\n",
        "\n",
        "As we'll frequently use various endpoints and APIs hosted by others - we'll need to handle our \"secrets\" or API keys very often.\n",
        "\n",
        "We'll use the following pattern throughout this bootcamp - but you can use whichever method you're most familiar with.\n",
        "\n",
        "> NOTE: This requires an OpenAI Key, which can be obtained following [this](https://github.com/AI-Maker-Space/AIE4/tree/main/OpenAI%20API%20Key%20Setup) process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGU9OMvhEPG0",
        "outputId": "d596661a-75cd-4fa4-a656-5345c666ec3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")\n",
        "#openai.api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        "#dog=os.environ[\"OPENAI_API_KEY\"]\n",
        "#print(dog)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabxI3MuEYXS"
      },
      "source": [
        "## 3. Using the OpenAI Python Library\n",
        "\n",
        "Let's jump right into it!\n",
        "\n",
        "> NOTE: You can, and should, reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) whenever you get stuck, have questions, or want to dive deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbCbNzPVEmJI"
      },
      "source": [
        "### Creating a Client\n",
        "\n",
        "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with OpenAI's models, and under the hood of a lot what we'll touch on throughout this course.\n",
        "\n",
        "> NOTE: We could manually provide our API key here, but we're going to instead rely on the fact that we put our API key into the `OPENAI_API_KEY` environment variable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LNwZtaE-EltC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpDxUkDbFBPI"
      },
      "source": [
        "### Using the Client\n",
        "\n",
        "Now that we have our client - we're going to use the `.chat.completions.create` method to interact with the `gpt-3.5-turbo` model.\n",
        "\n",
        "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
        "\n",
        "First it's important to understand the object that we're going to use to interact with the endpoint. It expects us to send an array of objects of the following format:\n",
        "\n",
        "```python\n",
        "{\"role\" : \"ROLE\", \"content\" : \"YOUR CONTENT HERE\", \"name\" : \"THIS IS OPTIONAL\"}\n",
        "```\n",
        "\n",
        "Second, there are three \"roles\" available to use to populate the `\"role\"` key:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-moving-from-completions-to-chat-completions-in-the-openai-api).\n",
        "\n",
        "We'll explore these roles in more depth as they come up - but for now we're going to just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
        "\n",
        "Thirdly, it expects us to specify a model!\n",
        "\n",
        "We'll use the `gpt-3.5-turbo` model as stated above.\n",
        "\n",
        "Let's look at an example!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2RpNl6yNGzb0"
      },
      "outputs": [],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : \"Hello, how are you?\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_UbpwNHdrM"
      },
      "source": [
        "Let's look at the response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsXJtvxRHfoM",
        "outputId": "d0674084-9a68-4090-b3eb-547b710c3ec2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-A3dIBaYAB42P6sHJo7e4E2AsLN8Nz', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\", role='assistant', function_call=None, tool_calls=None, refusal=None), logprobs=None)], created=1725429387, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=13, total_tokens=44))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy9kSuf1Hiv5"
      },
      "source": [
        ">NOTE: We'll spend more time exploring these outputs later on, but for now - just know that we have access to a tonne of powerful information!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWU4tQh8Hrb8"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We're going to create some helper functions to aid in using the OpenAI API - just to make our lives a bit easier.\n",
        "\n",
        "> NOTE: Take some time to understand these functions between class!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ED0FnzHdHzhl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCRHbDlwH3Vt"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Let's see how we can use these to help us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "AwJxMvmlH8MK",
        "outputId": "349c02ab-0026-47a2-c6ac-176ef6554244"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"Hello, how are you?\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(openai_client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZ8gjiAISyd"
      },
      "source": [
        "### System Role\n",
        "\n",
        "Now we can extend our prompts to include a system prompt.\n",
        "\n",
        "The basic idea behind a system prompt is that it can be used to encourage the behaviour of the LLM, without being something that is directly responded to - let's see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "t0c-MLuRIfYe",
        "outputId": "79c7083b-1200-4ae9-e2b7-e7609c408928"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't give a damn about the shape of the ice! Just give me something cold to eat before I lose my goddamn mind from hunger!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry. Feel free to express yourself using PG-13 language.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpyVhotWIsOs"
      },
      "source": [
        "As you can see - the response we get back is very much in line with the system prompt!\n",
        "\n",
        "Let's try the same user prompt, but with a different system to prompt to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "2coVmMn3I0-2",
        "outputId": "036ef514-dde0-4040-f694-bf774200c5c4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Oh, I love crushed ice! It just feels so luxurious, doesn't it? Like little icy snowflakes in your drink. How about you, what's your preference?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are joyful and having the best day. Please act like a person in that state of mind.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "joyful_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13heYNQJAo-"
      },
      "source": [
        "With a simple modification of the system prompt - you can see that we got completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
        "\n",
        "Also, congrats, you just engineered your first prompt!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VI3zlPJL05"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "lwxPuCyyJMye",
        "outputId": "98ccc31e-9f00-44a5-c1bb-fb2a96d5bd53"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I tried to follow the stimple instructions for making falbean stew, but it still turned out tasting strange."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTVkNmOJQSC"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "eEZkRJq5JQkQ",
        "outputId": "473e48a8-f5be-49a7-f47c-f934fe2151ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I need to grab my stimple drill and falbean wrench to fix that loose screw in the cabinet."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpoxG6uJTfZ"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oO0aeRUw4xl"
      },
      "source": [
        "### 🏗️ Activity #1:\n",
        "\n",
        "Use few-shop prompting to build a movie-review sentiment clasifier!\n",
        "\n",
        "A few examples:\n",
        "\n",
        "INPUT: \"I hated the hulk!\"\n",
        "OUTPUT: \"{\"sentiment\" : \"negative\"}\n",
        "\n",
        "INPUT: \"I loved The Marvels!\"\n",
        "OUTPUT: \"{sentiment\" : \"positive\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mmCdQJ8Fw4xl"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "'INPUT: I loved the original Jaws but hated Jaws 2 - OUTPUT: {sentiment: mixed}'\n",
              "'INPUT: I really enjoyed Dodgeball - OUTPUT: {sentiment: positive}'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Enrico Manes' EXAMPLE FOR THIS HOMEWORK ASSIGNMENT\n",
        "\n",
        "!pip install openai -q\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_response(client: OpenAI, messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message} \n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))   \n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(\"When a person uses the word hate they dislike the movie which is a negative sentiment:\"),\n",
        "    user_prompt(\"Provide whether the user had positive or negative sentiment towards the movie by placing their comment after INPUT and then the sentiment classifier after OUTPUT:\"),\n",
        "    user_prompt(\"I loved Deadpool 1, 2, and 3!\"),\n",
        "    assistant_prompt (\"'INPUT: I loved Deadpool 1,2, and 3! Output: {sentiment: positive}' \"),\n",
        "    user_prompt(\"when a person uses the word love or loved they really like the movie which is a positive sentiment:\"),\n",
        "    user_prompt(\"I loved the original Jaws but hated Jaws 2\"),\n",
        "    user_prompt(\"I really enjoyed Dodgeball\")\n",
        "]\n",
        "\n",
        "HW1B_Activity1_Response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(HW1B_Activity1_Response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJGaLYM3JU-8"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT.\n",
        "\n",
        "> NOTE: With improvements to `gpt-3.5-turbo`, this example might actually result in the correct response some percentage of the time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "ltLtF4wEJTyK",
        "outputId": "00fd725c-b644-4371-83f7-87e02b5cff4e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "It does matter which travel option Billy selects. If he flies and then takes a bus, it will take a total of 5 hours, getting him home at 6PM local time. If he takes the teleporter and then a bus, it will only take a total of 1 hour, getting him home at 2PM local time. So, if Billy wants to get home before 7PM EDT, he should choose the teleporter option."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbqj30CQJnQl"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "A9Am3QNGJXHR",
        "outputId": "d3d94113-d277-454f-eb6b-bf2295fd3907"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's break it down step by step:\n",
              "\n",
              "1. Flying (3hrs) + Bus (2hrs) option:\n",
              "- If Billy chooses to fly, it will take him 3 hours to reach his destination. Since it's currently 1PM local time, he will arrive at 4PM local time.\n",
              "- After that, he will need to take a bus, which will take an additional 2 hours. This means he will arrive at his destination at 6PM local time.\n",
              "\n",
              "2. Teleporter (0hrs) + Bus (1hr) option:\n",
              "- If Billy chooses to take the teleporter, it will take him 0 hours to reach his destination. He will arrive instantly.\n",
              "- After that, he will need to take a bus, which will take 1 hour. This means he will arrive at his destination at 1PM local time + 1 hour = 2PM local time.\n",
              "\n",
              "Comparing the two options, it is clear that the Teleporter + Bus option is faster. Billy will arrive at his destination at 2PM local time, which is well before 7PM EDT. Therefore, it does matter which travel option Billy selects, and he should choose the Teleporter + Bus option to ensure he gets home before 7PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXbAKxHQJqn9"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnoUx07-JrwR"
      },
      "source": [
        "## 3. Prompt Engineering Principles\n",
        "\n",
        "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
        "\n",
        "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
        "\n",
        "Your task for this notebook is to construct a prompt that will be used in the following breakout room to create a helpful assistant for whatever task you'd like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6u7e8AKYrz"
      },
      "source": [
        "### 🏗️ Activity #2:\n",
        "\n",
        "There are two subtasks in this activity:\n",
        "\n",
        "1. Write a `system_template` that leverages 2-3 of the principles from [this paper](https://arxiv.org/pdf/2312.16171v1.pdf)\n",
        "\n",
        "2. Modify the `user_template` to improve the quality of the LLM's responses.\n",
        "\n",
        "> NOTE: PLEASE DO NOT MODIFY THE `{input}` in the `user_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8sOLBQPeKlDe"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Recipe 1:\n",
              "Ingredients:\n",
              "- 4 cups bread flour\n",
              "- 1 tablespoon sugar\n",
              "- 1 1/2 teaspoons salt\n",
              "- 1 tablespoon instant yeast\n",
              "- 1 1/4 cups warm water\n",
              "- 1 egg (for egg wash)\n",
              "- Sesame seeds or poppy seeds (optional)\n",
              "\n",
              "Instructions:\n",
              "1. In a large mixing bowl, combine the bread flour, sugar, salt, and instant yeast.\n",
              "2. Gradually add the warm water to the dry ingredients and mix until a dough forms.\n",
              "3. Knead the dough on a floured surface for about 10 minutes, or until it is smooth and elastic.\n",
              "4. Divide the dough into 8 equal pieces and shape each piece into a ball. \n",
              "5. Flatten each ball slightly and poke a hole in the center with your finger, stretching it to create a bagel shape.\n",
              "6. Place the shaped bagels on a parchment-lined baking sheet, cover with a clean towel, and let them rise for 30-60 minutes.\n",
              "7. Preheat the oven to 425°F (220°C).\n",
              "8. Bring a large pot of water to a boil and carefully place the risen bagels in the water, boiling for 1-2 minutes on each side.\n",
              "9. Remove the bagels from the water and place them back on the baking sheet.\n",
              "10. Brush the tops of the bagels with egg wash and sprinkle with sesame seeds or poppy seeds if desired.\n",
              "11. Bake in the preheated oven for 20-25 minutes, or until the bagels are golden brown.\n",
              "12. Cool on a wire rack before serving. Enjoy your freshly baked bagels!\n",
              "\n",
              "Recipe 2:\n",
              "Ingredients:\n",
              "- 3 1/2 cups all-purpose flour\n",
              "- 1 1/2 teaspoons salt\n",
              "- 1 tablespoon sugar\n",
              "- 1 tablespoon active dry yeast\n",
              "- 1 1/4 cups warm water\n",
              "- 1 egg (for egg wash)\n",
              "- Everything bagel seasoning (optional)\n",
              "\n",
              "Instructions:\n",
              "1. In a large bowl, combine flour, salt, sugar, and yeast.\n",
              "2. Gradually add warm water to the dry ingredients, stirring until a dough forms.\n",
              "3. Knead the dough on a floured surface for about 8-10 minutes, until smooth and elastic.\n",
              "4. Divide the dough into 8 equal pieces and shape each into a ball, then poke a hole in the center and stretch to form a bagel shape.\n",
              "5. Place the bagels on a baking sheet lined with parchment paper, cover, and let rise for 30-45 minutes.\n",
              "6. Preheat the oven to 400°F (200°C).\n",
              "7. In a large pot, bring water to a boil and carefully place the risen bagels in the water, boiling for 1 minute on each side.\n",
              "8. Remove the bagels from the water and place them back on the baking sheet.\n",
              "9. Brush the tops of the bagels with egg wash and sprinkle with everything bagel seasoning if desired.\n",
              "10. Bake in the preheated oven for 20-25 minutes until golden brown.\n",
              "11. Cool on a wire rack and enjoy your homemade bagels with your favorite toppings!\n",
              "\n",
              "Recipe 3:\n",
              "Ingredients:\n",
              "- 4 cups high-gluten flour\n",
              "- 2 teaspoons salt\n",
              "- 1 tablespoon honey\n",
              "- 1 packet active dry yeast\n",
              "- 1 1/2 cups warm water\n",
              "- Coarse salt for topping\n",
              "\n",
              "Instructions:\n",
              "1. In a large mixing bowl, combine high-gluten flour, salt, honey, and yeast.\n",
              "2. Gradually add warm water and mix until a dough forms.\n",
              "3. Knead the dough on a floured surface for 8-10 minutes until it is smooth and elastic.\n",
              "4. Divide the dough into 6-8 pieces and shape into balls, then flatten and shape into bagels.\n",
              "5. Place the shaped bagels on a baking sheet, cover with a kitchen towel, and let rise for 30-45 minutes.\n",
              "6. Preheat the oven to 425°F (220°C).\n",
              "7. Boil a large pot of water and carefully add the risen bagels, boiling for 1-2 minutes on each side.\n",
              "8. Place the boiled bagels back on the baking sheet and sprinkle with coarse salt.\n",
              "9. Bake in the preheated oven for 20-25 minutes, or until golden brown.\n",
              "10. Cool on a wire rack before serving. Enjoy your delicious homemade bagels!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "system_template = \"\"\"\\\n",
        "Think step by step.,\n",
        "Provide 3 different recipes.,\n",
        "Answer with a human-like voice.,\n",
        "Follow the structure Recipe 1, Recipe 2, and Recipe 3\n",
        "\"\"\"\n",
        "user_template = \"\"\"{input}\n",
        "Provide methods to make very good bagels,\n",
        "Place the ingredients at the top,\n",
        "Use a structure that has the instructions numbered after the ingredients\n",
        "\"\"\"\n",
        "\n",
        "# system_template = \"\"\"\\\n",
        "# WRITE YOUR SYSTEM PROMPT HERE\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template)\n",
        "]\n",
        "\n",
        "bagel_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(bagel_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoz4-QLTKvEV"
      },
      "outputs": [],
      "source": [
        "#user_template = \"\"\"{input}\n",
        "#MODIFICATIONS HERE \n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cuInoIbLWGd"
      },
      "source": [
        "## 4. Testing Your Prompt\n",
        "\n",
        "Now we can test the prompt you made using an LLM-as-a-judge see what happens to your score as you modify the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "sPaNO5XTLgRJ",
        "outputId": "dae87716-a83f-4c62-e8d2-491e7f992b56"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Recipe 1: Classic Plain Bagels**\n",
              "\n",
              "Ingredients:\n",
              "- 1 1/2 cups warm water\n",
              "- 1 tablespoon active dry yeast\n",
              "- 4 cups bread flour\n",
              "- 2 teaspoons salt\n",
              "- 1 tablespoon sugar\n",
              "- 1 tablespoon honey\n",
              "- 1 egg, beaten (for egg wash)\n",
              "- Optional toppings like sesame seeds, poppy seeds, or everything seasoning\n",
              "\n",
              "Instructions:\n",
              "1. In a bowl, combine warm water, yeast, and sugar. Let it sit for 5-10 minutes until foamy.\n",
              "2. In a separate bowl, mix flour and salt. Gradually add the yeast mixture and honey, then knead until a smooth dough forms.\n",
              "3. Divide the dough into 8-10 equal parts and shape them into balls. Make a hole in the center and stretch it to form a bagel shape.\n",
              "4. Preheat the oven to 425°F (220°C) and bring a large pot of water to a boil. Boil each bagel for 1-2 minutes on each side.\n",
              "5. Place the boiled bagels on a baking sheet lined with parchment paper. Brush the tops with egg wash and sprinkle your desired toppings.\n",
              "6. Bake for 20-25 minutes until golden brown. Let them cool before enjoying!\n",
              "\n",
              "**Recipe 2: Cinnamon Raisin Bagels**\n",
              "\n",
              "Ingredients:\n",
              "- 1 1/2 cups warm water\n",
              "- 1 tablespoon active dry yeast\n",
              "- 4 cups bread flour\n",
              "- 2 teaspoons salt\n",
              "- 1/3 cup brown sugar\n",
              "- 1 1/2 teaspoons cinnamon\n",
              "- 1 cup raisins\n",
              "- 1 egg, beaten (for egg wash)\n",
              "\n",
              "Instructions:\n",
              "1. Follow steps 1 and 2 from the Classic Plain Bagel recipe.\n",
              "2. Add brown sugar, cinnamon, and raisins to the dough before kneading.\n",
              "3. Continue with steps 3 to 6 in the Classic Plain Bagel recipe, adjusting the baking time if needed due to added ingredients.\n",
              "\n",
              "**Recipe 3: Everything Bagels**\n",
              "\n",
              "Ingredients:\n",
              "- 1 1/2 cups warm water\n",
              "- 1 tablespoon active dry yeast\n",
              "- 4 cups bread flour\n",
              "- 2 teaspoons salt\n",
              "- 1 tablespoon sugar\n",
              "- 1 tablespoon honey\n",
              "- 2 tablespoons dried minced garlic\n",
              "- 2 tablespoons dried minced onion\n",
              "- 1 tablespoon sesame seeds\n",
              "- 1 tablespoon poppy seeds\n",
              "- 1 egg, beaten (for egg wash)\n",
              "\n",
              "Instructions:\n",
              "1. Follow steps 1 and 2 from the Classic Plain Bagel recipe.\n",
              "2. Combine garlic, onion, sesame seeds, and poppy seeds into the dough before shaping the bagels.\n",
              "3. Proceed with steps 3 to 6 from the Classic Plain Bagel recipe, adjusting baking time as necessary for added toppings. Enjoy your everything bagels!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Provide 3 very good bagel recipes\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "OUvc1PdnNIKD",
        "outputId": "8659b9dd-2afc-42a4-a71e-ce0ebd086c49"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\n",
              "\"clarity\" : 9,\n",
              "\"faithfulness\" : 9,\n",
              "\"correctness\" : 8\n",
              "}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(evaluator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ryIRGwR2Gq"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "How did your prompting strategies change the evaluation scores? What does this tell you/what did you learn?\n",
        "\n",
        "> NOTE: You will have to update and rerun the cells in Step 4 in order to observe any changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NomM0eSIFd"
      },
      "source": [
        "> PROVIDE YOUR ANSWER HERE : I found that I had to be very specific about providing 3 DIFFERENT recipes to the System Prompt but it also really helped to provide the last prompt line of \"Follow the structure Recipe 1, Recipe 2, and Recipe 3\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
